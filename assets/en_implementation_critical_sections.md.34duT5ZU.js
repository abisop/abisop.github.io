import{_ as t,c as i,al as s,o}from"./chunks/framework.NFAqBSgQ.js";const u=JSON.parse('{"title":"Critical Sections","description":"","frontmatter":{},"headers":[],"relativePath":"en/implementation/critical_sections.md","filePath":"en/implementation/critical_sections.md"}'),a={name:"en/implementation/critical_sections.md"};function n(r,e,c,l,d,h){return o(),i("div",null,e[0]||(e[0]=[s(`<h1 id="critical-sections" tabindex="-1">Critical Sections <a class="header-anchor" href="#critical-sections" aria-label="Permalink to &quot;Critical Sections&quot;">​</a></h1><h2 id="types-and-effects-of-critical-sections" tabindex="-1">Types and Effects of Critical Sections <a class="header-anchor" href="#types-and-effects-of-critical-sections" aria-label="Permalink to &quot;Types and Effects of Critical Sections&quot;">​</a></h2><p>A critical section is a short sequence of code where exclusive execution is assured by globally disabling other activities while that code sequence executes. When we discuss critical sections here we really refer to one of two mechanisms:</p><ul><li><strong>Critical Section proper</strong> A critical section is established by calling <code>enter_critical_section()</code>; the code sequence exits the critical section by calling <code>leave_critical_section()</code>. For the single CPU case, this amounts to simply disabling interrupts but is more complex in the SMP case where spinlocks are also involved.</li><li><strong>Disabling Pre-emption</strong> This is a related mechanism that is lumped into this discussion because of the similarity of its effects on the system. When pre-emption is disabled (via <code>sched_lock()</code>), interrupts remain enabled, but context switches may not occur; the current task is locked in place and cannot be suspended until the scheduler is unlocked (via <code>sched_unlock()</code>).</li></ul><p>The use of either mechanism will always harm real-time performance. The effects of critical sections on real-time performance is discussed in [[/implementation/preemption_latency]{.title-ref}.]([/implementation/preemption_latency]{.title-ref}..md) The end result is that a certain amount of <strong>jitter</strong> is added to the real-time response.</p><p>Critical sections cannot be avoided within the OS and, as a consequence, a certain amount of &quot;jitter&quot; in the response time is expected. The important thing is to monitor the maximum time that critical sections are in place in order to manage that jitter so that the variability in response time is within an acceptable range.</p><p>NOTE: This discussion applies to Normal interrupt processing. Most of this discussion does not apply to [[/guide](]{.title-ref}/guide.md)s/zerolatencyinterrupts\`. Those interrupts are not masked in the same fashion and none of the issues address in this page apply to those interrupts.</p><h2 id="single-cpu-critical-sections" tabindex="-1">Single CPU Critical Sections <a class="header-anchor" href="#single-cpu-critical-sections" aria-label="Permalink to &quot;Single CPU Critical Sections&quot;">​</a></h2><h3 id="os-interfaces" tabindex="-1">OS Interfaces <a class="header-anchor" href="#os-interfaces" aria-label="Permalink to &quot;OS Interfaces&quot;">​</a></h3><p>Before we talk about SMP Critical Sections let&#39;s first review the internal OS interfaces available and what they do in the single CPU case:</p><ul><li><code>up_irq_save()</code> (and its companion, <code>up_irq_restore()</code>). These simple interfaces just enable and disable interrupts globally. This is the simplest way to establish a critical section in the single CPU case. It does have side-effects to real-time behavior as discussed elsewhere.</li><li><code>up_irq_save()</code> should never be called directly, however. Instead, the wrapper macros enter_critical_section() (and its companion <code>leave_critical_section()</code>) or <code>spin_lock_irqsave()</code> (and <code>spin_unlock_irqrestore()</code>) should be used. In the single CPU case, these macros are defined to be simply <code>up_irq_save()</code> (or <code>up_irq_save()</code>). Rather than being called directly, they should always be called indirectly through these macros so that the code will function in the SMP environment as well.</li><li>Finally, there is <code>sched_lock()</code> (and <code>sched_unlock()</code>) that disable (and enable) pre-emption. That is, <code>sched_lock()</code> will lock your kernel thread in place and prevent other tasks from running. Interrupts are still enabled, but other tasks cannot run.</li></ul><h3 id="using-sched-lock-for-critical-sections-don-t" tabindex="-1">Using sched_lock() for Critical Sections -- <strong>DON&#39;T</strong> <a class="header-anchor" href="#using-sched-lock-for-critical-sections-don-t" aria-label="Permalink to &quot;Using sched\\_lock() for Critical Sections -- **DON\\&#39;T**&quot;">​</a></h3><p>In the single CPU case, <code>sched_lock()</code> can do a pretty good job of establishing a critical section too. After all, if no other tasks can run on the single CPU, then that task has pretty much exclusive access to all resources (provided that those resources are not shared with interrupt handlers). However, <code>sched_lock()</code> must never be used to establish a critical section because it does not work the same way in the SMP case. In the SMP case, locking the scheduer does not provide any kind of exclusive access to resources. Tasks running on other CPUs are still free to do whatever they wish.</p><h2 id="smp-critical-sections" tabindex="-1">SMP Critical Sections <a class="header-anchor" href="#smp-critical-sections" aria-label="Permalink to &quot;SMP Critical Sections&quot;">​</a></h2><h3 id="up-irq-save-and-up-irq-restore" tabindex="-1"><code>up_irq_save()</code> and <code>up_irq_restore()</code> <a class="header-anchor" href="#up-irq-save-and-up-irq-restore" aria-label="Permalink to &quot;\`up_irq_save()\` and \`up_irq_restore()\`&quot;">​</a></h3><p>As mentioned, <code>up_irq_save()</code> and <code>up_irq_restore()</code> should never be called directly. That is because the behavior is different in multiple CPU systems. In the multiple CPU case, these functions only enable (or disable) interrupts on the local CPU. They have no effect on interrupts in the other CPUs and hence really accomplish very little. Certainly they do not provide a critical section in any sense.</p><h3 id="enter-critical-section-and-leave-critical-section" tabindex="-1"><code>enter_critical_section()</code> and <code>leave_critical_section()</code> <a class="header-anchor" href="#enter-critical-section-and-leave-critical-section" aria-label="Permalink to &quot;\`enter_critical_section()\` and \`leave_critical_section()\`&quot;">​</a></h3><p><strong>spinlocks</strong></p><p>In order to establish a critical section, we also need to employ spinlocks. Spins locks are simply loops that execute in one processor. If processor A sets spinlock x, then processor B would have to wait for the spinlock like:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>while (test_and_set(x))</span></span>
<span class="line"><span> {</span></span>
<span class="line"><span> }</span></span></code></pre></div><p>Where test and set is an atomic operation that sets the value of a memory location but also returns its previous value. Here we are talking about atomic in terms of memory bus operations: The testing and setting of the memory location must be atomic with respect to other bus operations. Special hardware support of some kind is necessary to implement <code>test_and_set()</code> logic.</p><p>When Task A released the lock x, Task B will successfully take the spinlock and continue.</p><p><strong>Implementation</strong></p><p>Without going into the details of the implementation of <code>enter_critical_section()</code> suffice it to say that it (1) disables interrupts on the local CPU and (2) uses spinlocks to assure exclusive access to a code sequence across all CPUs.</p><p>NOTE that a critical section is indeed created: While within the critical section, the code does have exclusive access to the resource being protected. However the behavior is really very different:</p><ul><li>In the single CPU case, disable interrupts stops all possible activity from any other task. The single CPU becomes single threaded and un-interruptible.</li><li>In the SMP case, tasks continue to run on other CPUs. It is only when those other tasks attempt to enter a code sequence protected by the critical section that those tasks on other CPUs will be stopped. They will be stopped waiting on a spinlock.</li></ul><h3 id="spin-lock-irqsave-and-spin-unlock-irqrestore" tabindex="-1"><code>spin_lock_irqsave()</code> and <code>spin_unlock_irqrestore()</code> <a class="header-anchor" href="#spin-lock-irqsave-and-spin-unlock-irqrestore" aria-label="Permalink to &quot;\`spin_lock_irqsave()\` and \`spin_unlock_irqrestore()\`&quot;">​</a></h3><p><strong>Generic Interrupt Controller (GIC)</strong></p><p>ARM provides a special, optional sub-system called MPCore that provides multi-core support. One MPCore component is the Generic Interrupt Controller or GIC. The GIC supports 16 inter-processor interrupts and is a key component for implementing SMP on those platforms. The are called Software Generated Interrupts or SGIs.</p><p>One odd behavior of the GIC is that the SGIs cannot be disabled (at least not using the standard ARM global interrupt disable logic). So disabling local interrupts does not prevent these GIC interrupts.</p><p>This causes numerous complexities and significant overhead in establishing a critical section.</p><p><strong>ARMv7-M NVIC</strong></p><p>The GIC is available in all recent ARM architectures. However, most embedded ARM7-M multi-core CPUs just incorporate the inter-processor interrupts as a normal interrupt that is mask-able via the NVIC (each CPU will have its own NVIC).</p><p>This means in those cases, the critical section logic can be greatly simplified.</p><p><strong>Implementation</strong></p><p>For the case of the GIC with no support for disabling interrupts, <code>spin_lock_irqsave()</code> and <code>spin_unlock_irqstore()</code> are equivalent to <code>enter_critical_section()</code> and <code>leave_critical_section()</code>. In is only in the case where inter-processor interrupts can be disabled that there is a difference.</p><p>In that case, <code>spin_lock_irqsave()</code> will disable local interrupts and take a spinlock. This is really very simple and efficient implementation of a critical section.</p><p>There are two important things to note, however:</p><ul><li>The logic within this critical section must never suspend! For example, if code were to call <code>spin_lock_irqsave()</code> then <code>sleep()</code>, then the sleep would occur with the spinlock in the lock state and the whole system could be blocked. Rather, <code>spin_lock_irqsave()</code> can only be used with straight line code.</li><li>This is a different critical section than the one established via <code>enter_critical_section()</code>. Taking one critical section, does not prevent logic on another CPU from taking the other critical section and the result is that you make not have the protection that you think you have.</li></ul><h3 id="sched-lock-and-sched-unlock" tabindex="-1"><code>sched_lock()</code> and <code>sched_unlock()</code> <a class="header-anchor" href="#sched-lock-and-sched-unlock" aria-label="Permalink to &quot;\`sched_lock()\` and \`sched_unlock()\`&quot;">​</a></h3><p>Other than some details, the SMP <code>sched_lock()</code> works much like it does in the single CPU case. Here are the caveats:</p><ul><li>As in the single CPU case, the case that calls <code>sched_lock()</code> is locked in place and cannot be suspected.</li><li>However, tasks will continue to run on other CPUs so <code>sched_lock()</code> cannot be used as a critical section.</li><li>Tasks on other CPUs are also locked in place. However, they may opt to suspend themselves at any time (say, via <code>sleep()</code>). In that case, only the CPU&#39;s IDLE task will be permitted to run.</li></ul><h2 id="the-critical-section-monitor" tabindex="-1">The Critical Section Monitor <a class="header-anchor" href="#the-critical-section-monitor" aria-label="Permalink to &quot;The Critical Section Monitor&quot;">​</a></h2><h3 id="internal-os-hooks" tabindex="-1">Internal OS Hooks <a class="header-anchor" href="#internal-os-hooks" aria-label="Permalink to &quot;Internal OS Hooks&quot;">​</a></h3><p><strong>The Critical Section Monitor</strong></p><p>In order to measure the time that tasks hold critical sections, the OS supports a Critical Section Monitor. This is internal instrumentation that records the time that a task holds a critical section. It also records the amount of time that interrupts are disabled globally. The Critical Section Monitor then retains the maximum time that the critical section is in place, both per-task and globally. We also extend the critical section monitor to do task sched cost statistics, which can high effectively do cpuload statistic. In order to save not necessary cost when you only focus on specific feature, we isolate the crtimon features to difference configurations. Allow you only open some of the features to minimum the side effect of the performance etc.</p><p>The Critical Section Monitor is enabled with the following setting in the configurations:</p><pre><code>CONFIG_SCHED_CRITMONITOR=y
</code></pre><p>Enable sched critmon globally, all other features need this configuration as a prefix.</p><p><strong>Thread executing</strong>:</p><pre><code>CONFIG_SCHED_CRITMONITOR_MAXTIME_THREAD=0
</code></pre><ul><li>Default 0 to enable executing time statistic, and make it a source to support cpuload.</li><li>&gt; 0 to also do alert log when executing time above the configuration ticks.</li><li>-1 to disable thread executing time statistic feature.</li></ul><p>This method is <strong>recommend</strong> as a cpuload backend if you don&#39;t have more requirements in critmon. When disabled all other statistics in critmon, this method is a high efficiency way do cpu load statistic. As we did not add hooks to critical sections and preemption operations. Only have instructions when scheduler triggers context switch.</p><p><strong>Workq executing</strong>:</p><pre><code>CONFIG_SCHED_CRITMONITOR_MAXTIME_WQUEUE=-1
</code></pre><ul><li>Default -1 to disable workq queue max execution time</li><li>&gt; 0 to do alert log when workq executing time above the configuration ticks.</li></ul><p><strong>Preemption disabled time</strong>:</p><pre><code>CONFIG_SCHED_CRITMONITOR_MAXTIME_PREEMPTION=-1
</code></pre><ul><li>Default -1 to disable preemption disabled time statistic.</li><li>&gt;= 0 to enable preemption disabled time statistic, data will be in critmon procfs.</li><li>&gt; 0 to also do alert log when preemption disabled time above the configuration ticks.</li></ul><p><strong>Critical section entered time</strong>:</p><pre><code>CONFIG_SCHED_CRITMONITOR_MAXTIME_CSECTION=-1
</code></pre><ul><li>Default -1 to disable critical section entered time statistic.</li><li>&gt;= 0 to enable critical section entered time statistic, data will be in critmon procfs.</li><li>&gt; 0 to also do alert log when critical section entered time above the configuration ticks.</li></ul><p><strong>Irq executing time</strong>:</p><pre><code>CONFIG_SCHED_CRITMONITOR_MAXTIME_IRQ=-1
</code></pre><ul><li>Default -1 to disable irq executing time statistic.</li><li>&gt;= 0 to enable irq executing time statistic, data will be in critmon procfs.</li><li>&gt; 0 to also do alert log when irq executing time above the configuration ticks.</li></ul><p><strong>Wdog executing time</strong>:</p><pre><code>CONFIG_SCHED_CRITMONITOR_MAXTIME_WDOG=-1
</code></pre><ul><li>Default -1 to disable wdog executing time statistic.</li><li>&gt;= 0 to enable wdog executing time statistic, data will be in critmon procfs.</li><li>&gt; 0 to also do alert log when wdog executing time above the configuration ticks.</li></ul><p><strong>Perf Timers interface</strong></p><p>missing description for perf_xxx interface</p><p><strong>Per Thread and Global Critical Sections</strong></p><p>In NuttX critical sections are controlled on a per-task basis. For example, consider the following code sequence:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>irqstate_t flags = enter_critical_section();</span></span>
<span class="line"><span>sleep(5);</span></span>
<span class="line"><span>leave_critical_section(flags);</span></span></code></pre></div><p>The task, say Task A, establishes the critical section with <code>enter_critical_section()</code>, but when Task A is suspended by the <code>sleep(5)</code> statement, it relinquishes the critical section. The state of the system will then be determined by the next task to be resumed, say Task B: Typically, the next task will not be in a critical section and so the critical section is broken while the task sleeps. That critical section will be re-established when that Task A runs again after the sleep time expires.</p><p>However, if Task B that is resumed is also within a critical section, then the critical section will be extended even longer! This is why the global time that the critical section in place may be longer than any time that an individual thread holds the critical section.</p><h3 id="procfs" tabindex="-1">ProcFS <a class="header-anchor" href="#procfs" aria-label="Permalink to &quot;ProcFS&quot;">​</a></h3><p>The OS reports these maximum times via the ProcFS file system, typically mounted at <code>/proc</code>:</p><ul><li>The <code>/proc/&lt;ID&gt;/critmon</code> pseudo-file reports the per-thread maximum value for thread ID = &lt;ID&gt;. There is one instance of this critmon file for each active task in the system.</li><li>The <code>/proc/critmon</code> pseuo-file reports similar information for the global state of the CPU.</li></ul><p>The form of the output from the <code>/proc/&lt;ID&gt;/critmon</code> file is:</p><pre><code>X.XXXXXXXXX,X.XXXXXXXXX
</code></pre><p>Where <code>X.XXXXXXXXX</code> is the time in seconds with nanosecond precision (but not necessarily accuracy, accuracy is dependent on the timing clock source). The first number is the maximum time that the held pre-emption disabled; the second number number is the longest duration that the critical section was held.</p><p>This file cat be read from NSH like:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>nsh&gt; cat /proc/1/critmon</span></span>
<span class="line"><span>0.000009610,0.000001165</span></span></code></pre></div><p>The form of the output from the <code>/proc/critmon</code> file is similar:</p><pre><code>X,X.XXXXXXXXX,X.XXXXXXXXX
</code></pre><p>Where the first X is the CPU number and the following two numbers have the same interpretation as for <code>/proc/&lt;ID&gt;/critmon</code>. In the single CPU case, there will be one line in the pseudo-file with <code>X=0</code>; in the SMP case there will be multiple lines, one for each CPU.</p><p>This file can also be read from NSH:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>nsh&gt; cat /proc/critmon</span></span>
<span class="line"><span>0,0.000009902,0.000023590</span></span></code></pre></div><p>These statistics are cleared each time that the pseudo-file is read so that the reported values are the maximum since the last time that the ProcFS pseudo file was read.</p><h3 id="apps-system-critmon" tabindex="-1"><code>apps/system/critmon</code> <a class="header-anchor" href="#apps-system-critmon" aria-label="Permalink to &quot;\`apps/system/critmon\`&quot;">​</a></h3><p>Also available is a application daemon at <code>apps/system/critmon</code>. This daemon periodically reads the ProcFS files described above and dumps the output to stdout. This daemon is enabled with:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>nsh&gt; critmon_start</span></span>
<span class="line"><span>Csection Monitor: Started: 3</span></span>
<span class="line"><span>Csection Monitor: Running: 3</span></span>
<span class="line"><span>nsh&gt;</span></span>
<span class="line"><span>PRE-EMPTION CSECTION    PID   DESCRIPTION</span></span>
<span class="line"><span>MAX DISABLE MAX TIME</span></span>
<span class="line"><span>0.000100767 0.000005242  ---  CPU 0</span></span>
<span class="line"><span>0.000000292 0.000023590     0 Idle Task</span></span>
<span class="line"><span>0.000036696 0.000004078     1 init</span></span>
<span class="line"><span>0.000000000 0.000014562     3 Csection Monitor</span></span>
<span class="line"><span>...</span></span></code></pre></div><p>And can be stopped with:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>nsh&gt; critmon_stop</span></span>
<span class="line"><span>Csection Monitor: Stopping: 3</span></span>
<span class="line"><span>Csection Monitor: Stopped: 3</span></span></code></pre></div><h2 id="irq-monitor-and-worst-case-response-time" tabindex="-1">IRQ Monitor and Worst Case Response Time <a class="header-anchor" href="#irq-monitor-and-worst-case-response-time" aria-label="Permalink to &quot;IRQ Monitor and Worst Case Response Time&quot;">​</a></h2><p>The IRQ Monitor is additional OS instrumentation. A full discussion of the IRQ Monitor is beyond the scope of this page. Suffice it to say:</p><ul><li>The IRQ Monitor is enabled with <code>CONFIG_SCHED_IRQMONITOR=y</code>.</li><li>The data collected by the IRQ Monitor is provided in <code>/proc/irqs</code>.</li><li>This data can also be viewed using the <code>nsh&gt; irqinfo</code> command.</li><li>This data includes the number of interrupts received for each IRQ and the time required to process the interrupt, from entry into the attached interrupt handler until exit from the interrupt handler.</li></ul><p>From this information we can calculate the worst case response time from interrupt request until a task runs that can process the the interrupt. That worst cast response time, <code>Tresp</code>, is given by:</p><ul><li><code>Tresp1 = Tcrit + Tintr + C1</code></li><li><code>Tresp2 = Tintr + Tpreempt + C2</code></li><li><code>Tresp = MAX(Tresp1, Tresp2)</code></li></ul><p>Where:</p><ul><li><code>C1</code> and <code>C2</code> are unknown, irreducible constants that reflect such things as hardware interrupt latency and context switching time,</li><li><code>Tcrit</code> is the longest observed time within a critical section,</li><li><code>Tintr</code> is the time required for interrupt handler execution for the event of interest, and</li><li><code>Tpreempt</code> is the longest observed time with preemption disabled.</li></ul><p>NOTES:</p><ol><li><p>This calculation assumes that the task of interest is the highest priority task in the system. It does not consider the possibility of the responding task being delayed due to insufficient priority.</p></li><li><p>This calculation does not address the case where the interfering task has both preemption disabled and holds the critical section. Certainly Tresp1 is valid in this case, but Tresp2 is not. There might some additional, unmeasured delay after the interrupt and before the responding task can run depending on the order in which the critical section is released and preemption is re-enabled:</p><blockquote><ul><li>When the task leaves the critical section, the pending interrupt will execute immediately with or without preemption enabled.</li><li>If preemption is enabled first, then the will be no delay after the interrupt because preemption will be enabled when the interrupt returns.</li><li>If the task leaves critical section first, then there will be some small delay of unknown duration after the interrupts returns and before the responding task can run because preemption will be disabled when the interrupt returns.</li></ul></blockquote></li><li><p>This calculation does not address concurrent interrupts. All interrupts run at the same priority and if an interrupt request occurs while within an interrupt handler, then it must pend until completion of that interrupt. So perhaps the above formula for <code>Tresp1</code> should instead be the following? (This assumes that hardware arbitration is such that the interrupt of interest will be deferred by no more than one interrupt). Concurrent, nested interrupts might be better supported with prioritized. See more: [[/guide](]{.title-ref}/guide.md)s/nestedinterrupts\`.</p><blockquote><ul><li><p><code>Tresp1 = Tcrit + Tintrmax + Tintr + C1</code></p><p>Where:</p><ul><li><code>Tintrmax</code> is the longest interrupt processing time of all interrupt sources (excluding the interrupt for the event under consideration).</li></ul></li></ul></blockquote></li></ol><h3 id="what-can-you-do" tabindex="-1">What can you do? <a class="header-anchor" href="#what-can-you-do" aria-label="Permalink to &quot;What can you do?&quot;">​</a></h3><p>What can you do if the timing data indicates that you cannot meet your deadline? You have these options:</p><ol><li>Use these tools to find the exact function that holds the critical section or disables preemption too long. Then optimize that function so that it releases that resource sooner. Often critical sections are established over long sequences or code when they could be re-designed to use critical sections over shorter code sequences.</li><li>In some cases, use of critical sections or disabling of pre-emption could be replaced with a locking semaphore. The scope of the locking effect for the use of such locks is not global but is limited only to tasks that share the same resource. Critical sections should correctly be used only to protect resources that are shared between tasking level logic and interrupt level logic.</li><li>Switch to [[/guide](]{.title-ref}/guide.md)s/zerolatencyinterrupts\`. Those interrupts are not subject to most of the issues discussed in this page.</li></ol><p><strong>NOTE</strong></p><p>There are a few places in the OS were preemption is disabled via <code>sched_lock()</code> in order to establish a critical section. That is an incorrect use of <code>sched_lock()</code>. <code>sched_lock()</code> simply prevents the currently executing task from being suspended. For the case of the single CPU platform, that does effectively create a critical section: Since no other task can run, the locking task does have exclusive access to all resources that are not shared with interrupt level logic.</p><p>But in the multi-CPU SMP case that is not true. <code>sched_lock()</code> still keeps the current task running on CPU from being suspended, but it does not support any exclusivity in accesses because there will be other tasks running on other CPUs that may access the same resources.</p>`,109)]))}const m=t(a,[["render",n]]);export{u as __pageData,m as default};
